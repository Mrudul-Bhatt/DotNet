Let's explore these common scenarios related to concurrent collections and the principles behind their effective use in multi-threaded applications.

-----

### ðŸ”¹ You have multiple producer threads and one consumer. Which collection do you use, and why?

(Use `BlockingCollection<T>` with a `ConcurrentQueue<T>` â€” it supports thread-safe `Add()` and `Take()` with built-in blocking.)

**Explanation:**

For a multiple-producer, single-consumer (MPSC) or multiple-producer, multiple-consumer (MPMC) scenario, the best choice is `BlockingCollection<T>`, often wrapping a `ConcurrentQueue<T>` internally (which is the default behavior if you don't specify an underlying collection).

**Why `BlockingCollection<T>` is ideal:**

1.  **Thread Safety (No Race Conditions):**

      * `BlockingCollection<T>` is inherently thread-safe for both adding (producers) and taking (consumers) elements. You don't need to implement any explicit `lock` statements in your producer or consumer code, greatly simplifying concurrent programming and eliminating common sources of race conditions and deadlocks.

2.  **Built-in Blocking for Flow Control:**

      * **Consumers Block on Empty:** The `Take()` method (or `GetConsumingEnumerable()`) on `BlockingCollection<T>` will automatically *block* the consumer thread if the collection is empty. This prevents the consumer from busy-waiting or spinning uselessly, freeing up CPU cycles until an item becomes available.
      * **Producers Block on Full (with Bounded Capacity):** If you create `BlockingCollection<T>` with a `capacity` limit, the `Add()` method will automatically *block* the producer thread if the collection is full. This is crucial for back-pressure, preventing producers from overwhelming consumers or exhausting memory if processing is slower than production.

3.  **Clean Shutdown (`CompleteAdding()`):**

      * `BlockingCollection<T>` provides the `CompleteAdding()` method, which signals to consumers that no more items will be added. This allows consumers using `GetConsumingEnumerable()` to gracefully exit their loop once all existing items have been processed, without needing external flags or complex synchronization.

4.  **Flexibility (Underlying Collection):**

      * While `ConcurrentQueue<T>` is the default and most common underlying collection for FIFO behavior, `BlockingCollection<T>` can wrap any collection that implements `IProducerConsumerCollection<T>` (e.g., `ConcurrentStack<T>` for LIFO). This allows you to tailor the behavior (FIFO, LIFO) while retaining the blocking features.

**Example Setup:**

  * **Producers:** Call `blockingCollection.Add(item)` to add work.
  * **Consumer:** Calls `blockingCollection.Take()` or iterates with `blockingCollection.GetConsumingEnumerable()` to retrieve work. The consumer also needs to handle when producers signal completion (via `CompleteAdding()`).

This setup elegantly handles the synchronization and flow control without you having to write complex low-level threading primitives.

-----

### ðŸ”¹ You used `List<T>` in a shared in-memory cache accessed by multiple threads. Occasionally, you get exceptions. Whatâ€™s wrong?

(List\<T\> is not thread-safe â€” replace it with a `ConcurrentBag<T>` or `ConcurrentDictionary<K,V>` or synchronize access.)

**Explanation:**

The fundamental problem is that `List<T>` is **not thread-safe**. When multiple threads simultaneously try to modify a `List<T>` (e.g., `Add`, `Remove`, `Insert`, `Clear`) or even read while another thread is modifying, you introduce **race conditions**. These race conditions can lead to various unpredictable issues:

  * **`IndexOutOfRangeException`:** A thread might try to access an index that has just been removed by another thread, or an index that hasn't been validly written yet.
  * **`ArgumentOutOfRangeException`:** Similar to `IndexOutOfRangeException`, usually related to accessing invalid indices or ranges.
  * **`NullReferenceException`:** If a read happens while an element is being partially updated or shifted.
  * **Corrupted Data:** Items might be duplicated, lost, or overwritten incorrectly due to interleaved operations.
  * **Infinite Loops:** In rare cases during enumeration, the internal structure could be corrupted, leading to unexpected loop behavior.

**Solutions:**

1.  **Replace with a Concurrent Collection (Best Practice):**

      * **`ConcurrentDictionary<K,V>`:** This is usually the best choice for a *cache* scenario, as caches typically involve unique keys for lookup. It offers highly optimized concurrent `Add`, `Update`, `Get`, and `Remove` operations with O(1) average time complexity.
      * **`ConcurrentBag<T>`:** If your "cache" is simply a collection of items where order doesn't matter, and you primarily add and retrieve items (often for work distribution without keys), `ConcurrentBag<T>` is a good lock-free option. It's optimized for scenarios where different threads might add and remove items more or less independently.
      * **`ConcurrentQueue<T>` or `ConcurrentStack<T>`:** If your cache has a specific FIFO or LIFO access pattern.

2.  **Synchronize Access with `lock` (If changing collection type is not an option, but less performant):**

      * You could put `lock` statements around *every* access (read or write) to the `List<T>`. This ensures only one thread can operate on the list at a time.
      * **Drawbacks:** This significantly reduces concurrency, as threads will constantly block each other. It's often slower than concurrent collections for high contention scenarios, and it's prone to deadlocks if not implemented carefully.

    <!-- end list -->

    ```csharp
    // Problematic (as described in question):
    List<int> sharedCache = new List<int>(); // NOT THREAD-SAFE

    // Fixed (using ConcurrentDictionary for a cache scenario):
    ConcurrentDictionary<string, string> userCache = new ConcurrentDictionary<string, string>();
    userCache.TryAdd("user1", "data1");
    // userCache.Add("user1", "data1"); // This would throw if "user1" already exists

    // Fixed (synchronizing List<T> with lock - generally less optimal for high concurrency)
    List<int> synchronizedList = new List<int>();
    private readonly object _listLock = new object();

    public void AddToSynchronizedList(int item)
    {
        lock (_listLock) // Every access must be locked
        {
            synchronizedList.Add(item);
        }
    }

    public int GetFromSynchronizedList(int index)
    {
        lock (_listLock) // Every access must be locked
        {
            return synchronizedList[index];
        }
    }
    ```

**In summary, `List<T>` is designed for single-threaded or externally synchronized access. For shared in-memory caches, `ConcurrentDictionary<K,V>` is almost always the best choice due to its performance and built-in thread safety for key-value operations.**

-----

### ðŸ”¹ Your `ConcurrentDictionary<K,V>` throws when using `Add()`. Whatâ€™s the fix?

(Use `TryAdd()` or `AddOrUpdate()` â€” thread-safe methods that avoid exceptions during race conditions.)

**Explanation:**

The `Add()` method on `ConcurrentDictionary<K,V>` (like on a regular `Dictionary<K,V>`) will throw an `ArgumentException` if you try to add a key that already exists. While `ConcurrentDictionary` is thread-safe, this specific behavior of `Add()` remains consistent with its non-concurrent counterpart.

In a multi-threaded environment, if two threads try to `Add()` the *same* key, one will succeed, and the other will cause an `ArgumentException`. This is a race condition leading to an exception.

To handle this gracefully and in a thread-safe manner *without exceptions*, `ConcurrentDictionary<K,V>` provides specialized methods:

1.  **`TryAdd(TKey key, TValue value)`:**

      * Attempts to add the key-value pair.
      * Returns `true` if the key-value pair was added successfully (i.e., the key didn't exist).
      * Returns `false` if the key already exists (no exception is thrown, and the existing value is retained).
      * This is useful when you want to add an item only if it's new.

2.  **`AddOrUpdate(TKey key, TValue addValue, Func<TKey, TValue, TValue> updateValueFactory)`:**

      * This is the most powerful and flexible method for atomic additions or updates.
      * If the `key` **does not exist**, it adds the `key` with `addValue`.
      * If the `key` **does exist**, it calls the `updateValueFactory` delegate with the existing key and value, and uses the delegate's return value to update the entry. This operation is performed atomically.
      * This is ideal when you want to ensure a key is present and potentially modify its value if it already exists, or initialize it if it doesn't.

3.  **`GetOrAdd(TKey key, Func<TKey, TValue> valueFactory)`:**

      * Attempts to get the value associated with the specified `key`.
      * If the `key` **exists**, it returns the existing value.
      * If the `key` **does not exist**, it adds the `key` with the value produced by the `valueFactory` delegate, and then returns that new value. This is also atomic.
      * This is perfect for caching scenarios where you want to retrieve an item if it's in the cache, or compute/load it and add it to the cache if it's not.

**Code Example:**

```csharp
using System;
using System.Collections.Concurrent;
using System.Threading.Tasks;

public static class ConcurrentDictionaryAddFixExample
{
    public static void Run()
    {
        Console.WriteLine("--- ConcurrentDictionary Add Fix Example ---");

        ConcurrentDictionary<string, int> userScores = new ConcurrentDictionary<string, int>();

        // Simulate multiple threads trying to add/update scores for the same users
        Parallel.For(0, 10, i =>
        {
            string userName = "User" + (i % 3); // A few common users to create contention

            // Using AddOrUpdate: If user exists, increment score; else, set to 1
            int newScore = userScores.AddOrUpdate(
                userName,
                1, // Value to add if key is new
                (key, existingScore) => existingScore + 1 // How to update if key exists
            );
            Console.WriteLine($"Thread {Task.CurrentId}: {userName} score is now {newScore}");

            // Using TryAdd: Only adds if the key doesn't exist.
            // Useful for ensuring a default state without overwriting.
            if (userScores.TryAdd("NewUser" + i, 100))
            {
                Console.WriteLine($"Thread {Task.CurrentId}: Added NewUser{i}");
            }
            else
            {
                Console.WriteLine($"Thread {Task.CurrentId}: NewUser{i} already exists, not re-added.");
            }

            // Using GetOrAdd: Gets existing value or adds a new one
            int retrievedOrAddedScore = userScores.GetOrAdd(
                "DefaultUser",
                () => 0 // Value to add if key is new (using a lambda for factory)
            );
            Console.WriteLine($"Thread {Task.CurrentId}: DefaultUser score (GetOrAdd) is {retrievedOrAddedScore}");
        });

        Console.WriteLine("\nFinal User Scores:");
        foreach (var entry in userScores)
        {
            Console.WriteLine($"- {entry.Key}: {entry.Value}");
        }

        Console.WriteLine("--- End ConcurrentDictionary Add Fix Example ---");
    }
}
```

-----

### ðŸ”¹ How do `ConcurrentQueue<T>` and `ConcurrentStack<T>` behave under high-concurrency scenarios?

(They avoid locking through atomic operations (e.g., `Interlocked`), ensuring low-latency thread-safe push/pop or enqueue/dequeue.)

**Explanation:**

`ConcurrentQueue<T>` and `ConcurrentStack<T>` are designed for very high-concurrency scenarios and achieve their performance and thread safety primarily through **lock-free algorithms** using low-level **atomic operations**.

**Key Mechanism: Atomic Operations (`Interlocked`) and Spin-Waiting:**

  * Instead of using traditional coarse-grained locks (`Monitor.Enter`, `lock` keyword) that put threads to sleep and context-switch, these concurrent collections leverage CPU-level instructions that guarantee atomicity for specific operations (like reading, writing, or comparing and exchanging a value).
  * The `System.Threading.Interlocked` class in C\# provides managed wrappers for these atomic operations (e.g., `Interlocked.CompareExchange`, `Interlocked.Increment`, `Interlocked.Decrement`).
  * **How it works (simplified):**
    1.  A thread attempts to perform an operation (e.g., enqueue an item, pop an item).
    2.  It uses an atomic operation (like `CompareExchange`) to modify shared pointers or data.
    3.  If the atomic operation succeeds (meaning no other thread interfered), the operation is complete.
    4.  If the atomic operation fails (meaning another thread modified the data concurrently), the thread will typically **spin-wait** (repeatedly try the operation in a tight loop) or yield its time slice briefly, and then retry the operation. It doesn't acquire a heavy lock.

**Benefits under High Concurrency:**

1.  **Low Latency:** Since threads don't get blocked and put to sleep by the operating system (which is expensive due to context switching), operations are typically completed very quickly.
2.  **High Throughput:** Multiple threads can often make progress concurrently without waiting for each other on a lock, leading to a much higher rate of successful operations per unit of time.
3.  **No Deadlocks:** Lock-free algorithms inherently avoid deadlocks because there are no locks to acquire in conflicting orders.
4.  **No Lock Convoy:** Traditional locks can suffer from "lock convoys" where threads queue up to acquire a lock, even after the original thread releases it. Lock-free approaches mitigate this.

**Trade-offs (less relevant for common use but good to know):**

  * **Complexity:** Implementing truly lock-free algorithms is extremely complex and error-prone; hence, we use the pre-built `ConcurrentQueue` and `ConcurrentStack`.
  * **Busy-Waiting (Spinning):** While efficient for short waits, excessive spin-waiting on heavily contended resources can consume CPU cycles without doing useful work, which is why implementations often combine spinning with brief yields.

**In essence, `ConcurrentQueue<T>` and `ConcurrentStack<T>` are highly optimized for scenarios where many threads are constantly adding and removing items, providing excellent performance characteristics by minimizing the overhead associated with traditional locking.**

-----

### ðŸ”¹ You build a logging service using `ConcurrentQueue<T>` for log buffering. Over time, you see memory increase. Why?

(Queue not being drained â€” add a dedicated consumer/dequeue mechanism or use `BlockingCollection<T>` with bounded capacity.)

**Explanation:**

If you're using a `ConcurrentQueue<T>` as a buffer for a logging service and observe increasing memory usage, it almost certainly indicates a **producer-consumer imbalance**: your log messages are being produced (enqueued) faster than they are being consumed (dequeued and written to a sink like a file or database).

**The Problem:**

  * **Unbounded Queue:** `ConcurrentQueue<T>` is an *unbounded* collection by default. It will continue to grow as long as producers add items and the consumer cannot keep up.
  * **Memory Accumulation:** Each log message added to the queue consumes memory. If the rate of production consistently exceeds the rate of consumption, the queue will grow indefinitely, eventually leading to an `OutOfMemoryException`.
  * **No Back-Pressure:** `ConcurrentQueue<T>.Enqueue()` is a non-blocking operation. Producers will never be slowed down if the consumer is falling behind, meaning they will continue to add items regardless of the consumer's capacity.

**How to Fix It:**

You need to ensure the consumer can keep up, or introduce **back-pressure** to slow down the producers.

1.  **Ensure a Dedicated and Efficient Consumer:**

      * **Dedicated Thread/Task:** Have at least one dedicated long-running thread or `Task` whose sole job is to continuously `TryDequeue` messages from the queue and write them to the log sink.
      * **Batching:** Instead of writing one log message at a time, consider batching them. Dequeue multiple messages at once and write them in a single I/O operation. This can significantly improve performance for disk or network writes.
      * **Asynchronous I/O:** Use `async`/`await` and asynchronous I/O operations (e.g., `File.AppendAllTextAsync`, `StreamWriter.WriteAsync`) in your consumer to prevent the consumer thread from blocking while waiting for I/O, allowing it to pick up more messages faster.
      * **Error Handling/Retry:** Ensure your consumer handles potential I/O errors gracefully, so it doesn't stop processing due to transient issues.

2.  **Use `BlockingCollection<T>` with Bounded Capacity (Recommended for Back-Pressure):**

      * This is the most robust solution for preventing unbounded growth.
      * Initialize `BlockingCollection<T>` with a `capacity` limit (e.g., `new BlockingCollection<LogEntry>(1000)`).
      * When producers call `blockingCollection.Add(item)`, if the queue is full, the `Add()` call will **block** until space becomes available (i.e., a consumer dequeues an item).
      * This provides automatic back-pressure: producers are implicitly slowed down if the consumer falls behind, preventing memory exhaustion.

    **Example using `BlockingCollection<T>` for logging:**

    ```csharp
    using System;
    using System.Collections.Concurrent;
    using System.Threading;
    using System.Threading.Tasks;
    using System.Diagnostics; // For Stopwatch

    public class LogEntry { public string Message { get; set; } public DateTime Timestamp { get; set; } }

    public class LoggingService : IDisposable
    {
        private BlockingCollection<LogEntry> _logQueue;
        private CancellationTokenSource _cts;
        private Task _consumerTask;

        public LoggingService(int queueCapacity = 1000)
        {
            _logQueue = new BlockingCollection<LogEntry>(queueCapacity);
            _cts = new CancellationTokenSource();
            _consumerTask = Task.Run(() => ConsumeLogs(_cts.Token));
            Console.WriteLine($"[LoggingService] Initialized with queue capacity: {queueCapacity}");
        }

        public void Log(string message)
        {
            if (_cts.IsCancellationRequested) return; // Don't log if shutting down

            LogEntry entry = new LogEntry { Message = message, Timestamp = DateTime.Now };
            try
            {
                // This Add() will block if queue is full (back-pressure)
                _logQueue.Add(entry);
                // Console.WriteLine($"[Producer] Logged: {message}");
            }
            catch (InvalidOperationException) // Thrown if CompleteAdding() was called
            {
                Console.WriteLine($"[Producer] Log queue is closed, could not add: {message}");
            }
        }

        private void ConsumeLogs(CancellationToken cancellationToken)
        {
            Console.WriteLine("[Consumer] Consumer task started.");
            try
            {
                // GetConsumingEnumerable blocks until item available or CompleteAdding() called
                foreach (var entry in _logQueue.GetConsumingEnumerable(cancellationToken))
                {
                    // Simulate writing to disk (e.g., File.AppendAllText or a database)
                    // For demo, just print
                    Console.WriteLine($"[Consumer] Writing log: [{entry.Timestamp:HH:mm:ss.fff}] {entry.Message}");
                    Thread.Sleep(5); // Simulate I/O delay
                }
            }
            catch (OperationCanceledException)
            {
                Console.WriteLine("[Consumer] Consumer task cancelled.");
            }
            finally
            {
                Console.WriteLine("[Consumer] Consumer task finished.");
            }
        }

        public void Dispose()
        {
            Console.WriteLine("[LoggingService] Shutting down...");
            _logQueue.CompleteAdding(); // Tell producers no more adds, and consumers to finish
            _cts.Cancel(); // Signal cancellation
            _consumerTask.Wait(TimeSpan.FromSeconds(5)); // Wait for consumer to finish gracefully
            _logQueue.Dispose(); // Dispose the collection
            _cts.Dispose();
        }
    }

    public static class LoggingServiceExample
    {
        public static void Run()
        {
            Console.WriteLine("--- Logging Service with BlockingCollection Example ---");

            using (LoggingService logger = new LoggingService(queueCapacity: 10)) // Small capacity for demo
            {
                // Simulate fast producers
                Parallel.For(0, 50, i =>
                {
                    logger.Log($"Event {i}");
                    Thread.Sleep(1); // Small delay
                });

                Console.WriteLine("\n[Main] All producers finished adding messages.");
                Thread.Sleep(200); // Give consumer a chance to catch up
            } // Dispose will be called here

            Console.WriteLine("--- End Logging Service with BlockingCollection Example ---");
        }
    }
    ```

-----

### ðŸ”¹ You need to atomically add or update a counter for user sessions in a multi-threaded app. What method should you use?

(Use `ConcurrentDictionary<K,V>.AddOrUpdate()` or `GetOrAdd()` with a factory lambda.)

**Explanation:**

When you need to atomically add or update a value associated with a key in a multi-threaded environment, `ConcurrentDictionary<K,V>` is the right choice, specifically its `AddOrUpdate()` or `GetOrAdd()` methods.

**Why these methods:**

  * **Atomicity:** These methods guarantee that the entire operation (checking for existence and then adding or updating) is performed as a single, indivisible unit. This prevents race conditions where one thread might check if a key exists, another thread adds it, and then the first thread tries to add it again, leading to an exception or inconsistent state.
  * **No Explicit Locks:** You don't need to put `lock` statements around these operations in your application code, as the dictionary handles the synchronization internally.

**Detailed Use:**

1.  **`AddOrUpdate(TKey key, TValue addValue, Func<TKey, TValue, TValue> updateValueFactory)`:**

      * **Scenario:** You want to either set a new value if the key is absent, or perform a computation to derive a new value if the key exists.
      * **Example (Incrementing a counter):** If the key doesn't exist, set the counter to 1. If it exists, increment the existing counter by 1.

    <!-- end list -->

    ```csharp
    // To increment a counter
    userSessionCounters.AddOrUpdate(
        sessionId,
        1, // Initial value if new session
        (key, existingCount) => existingCount + 1 // How to update existing count
    );
    ```

2.  **`GetOrAdd(TKey key, Func<TKey, TValue> valueFactory)`:**

      * **Scenario:** You want to retrieve a value if it exists, otherwise create it with a default/initial value and add it. The `valueFactory` is *only* executed if the key needs to be added.
      * **Example (Ensuring counter exists and then incrementing):** This is often used in a two-step process if `AddOrUpdate`'s `updateValueFactory` is insufficient, or if you need to perform other operations after getting the value.

    <!-- end list -->

    ```csharp
    // To ensure a counter exists, then increment it (requires two steps for ++, but GetOrAdd is atomic for the add part)
    int currentCount = userSessionCounters.GetOrAdd(sessionId, 0); // Get existing or add 0
    // Now you have the count, you can increment it externally or use Interlocked.Increment
    userSessionCounters[sessionId] = currentCount + 1; // This is not atomic for ConcurrentDictionary
    // For atomic increment, AddOrUpdate is better.
    // GetOrAdd is great for ensuring an entry is there, e.g., for complex objects.

    // A better way to use GetOrAdd for a complex object whose creation is expensive
    // and then work with the object:
    ConcurrentDictionary<string, UserSession> activeSessions = new ConcurrentDictionary<string, UserSession>();
    UserSession session = activeSessions.GetOrAdd(
        sessionId,
        (sId) =>
        {
            Console.WriteLine($"[Thread {Task.CurrentId}] Creating new session for {sId}");
            return new UserSession { SessionId = sId, LastActivity = DateTime.UtcNow };
        }
    );
    session.LastActivity = DateTime.UtcNow; // Now work with the retrieved/new session
    ```

**Example for Atomic Counter:**

```csharp
using System;
using System.Collections.Concurrent;
using System.Threading.Tasks;
using System.Threading; // For Interlocked if needed

public class UserSession // A simplified session object
{
    public string Id { get; set; }
    public int PageViews { get; set; }
    // More session data
}

public static class AtomicCounterExample
{
    public static void Run()
    {
        Console.WriteLine("--- Atomic Counter for User Sessions Example ---");

        ConcurrentDictionary<string, int> sessionPageViews = new ConcurrentDictionary<string, int>();

        // Simulate multiple threads incrementing page views for different sessions
        Parallel.For(0, 1000, i =>
        {
            string sessionId = "Session" + (i % 50); // 50 unique sessions
            int currentViews = sessionPageViews.AddOrUpdate(
                sessionId,
                1, // If session is new, start with 1 page view
                (key, existingViews) => existingViews + 1 // If session exists, increment views
            );
            // Console.WriteLine($"Thread {Task.CurrentId}: {sessionId} has {currentViews} page views.");
        });

        Console.WriteLine("\nFinal Page Views per Session:");
        foreach (var entry in sessionPageViews)
        {
            Console.WriteLine($"- {entry.Key}: {entry.Value}");
        }

        Console.WriteLine("--- End Atomic Counter for User Sessions Example ---");
    }
}
```

-----

### ðŸ”¹ You use `BlockingCollection<T>` with bounded capacity. What happens when the producer adds more than the capacity?

(The `Add()` call blocks until space becomes available â€” prevents overloading the queue.)

**Explanation:**

This is the core behavior and a key advantage of `BlockingCollection<T>` when configured with a `capacity` limit.

  * **Blocking `Add()`:** If you create `BlockingCollection<T>` with a positive `capacity` (e.g., `new BlockingCollection<T>(100)`), and producers attempt to add an item when the collection's `Count` is already equal to its `capacity`, the `Add()` call will **block** the calling producer thread.
  * **Waiting for Space:** The producer thread will remain blocked, waiting, until a consumer thread calls `Take()` (or `TryTake()`) and creates space within the collection. Once space is available, the blocked producer's `Add()` operation can complete.
  * **Back-Pressure:** This mechanism is known as **back-pressure**. It automatically slows down or pauses the producers when the consumers are falling behind or when the buffer is full. This is crucial for:
      * **Preventing Memory Exhaustion:** It stops the buffer from growing indefinitely, protecting your application from `OutOfMemoryException` if producers are much faster than consumers.
      * **Resource Control:** It ensures that producers don't generate more work than the downstream processing pipeline can handle, preventing resource bottlenecks (e.g., excessive database connections, file handles, CPU load on consumer).
      * **Flow Control:** It provides a natural and efficient way to regulate the flow of items through a pipeline without complex manual signaling or polling.

**Example Revisited (from `BlockingCollection<T>` section):**

```csharp
using System;
using System.Collections.Concurrent;
using System.Threading;
using System.Threading.Tasks;

public static class BlockingCollectionBoundedExample
{
    public static void Run()
    {
        Console.WriteLine("--- BlockingCollection<T> Bounded Capacity Example ---");

        // Create a BlockingCollection with a bounded capacity of 5
        using (BlockingCollection<int> dataItems = new BlockingCollection<int>(capacity: 5))
        {
            // Producer Task
            Task producer = Task.Run(() =>
            {
                for (int i = 0; i < 15; i++) // Try to add more than capacity (15 items)
                {
                    Console.WriteLine($"[Producer] Trying to add: {i} (Queue Count: {dataItems.Count})");
                    dataItems.Add(i); // This will block if capacity is reached
                    Console.WriteLine($"[Producer] Successfully added: {i}");
                    Thread.Sleep(100); // Simulate some work after adding
                }
                dataItems.CompleteAdding(); // Signal that no more items will be added
                Console.WriteLine("[Producer] Finished adding items and signaled completion.");
            });

            // Consumer Task
            Task consumer = Task.Run(() =>
            {
                // Give producer a head start to fill the queue
                Thread.Sleep(500);
                Console.WriteLine("\n[Consumer] Starting to consume...");
                foreach (int item in dataItems.GetConsumingEnumerable())
                {
                    Console.WriteLine($"[Consumer] Took: {item} (Queue Count: {dataItems.Count})");
                    Thread.Sleep(400); // Simulate slower processing time than production
                }
                Console.WriteLine("[Consumer] Finished consuming all items.");
            });

            // Wait for both tasks to complete
            Task.WaitAll(producer, consumer);
        }

        Console.WriteLine("--- End BlockingCollection<T> Bounded Capacity Example ---");
    }
}
```

In the output of this example, you would observe that the `[Producer] Adding:` messages would stop after 5 items are successfully added, and only resume once the consumer starts taking items, demonstrating the blocking behavior and effective back-pressure.